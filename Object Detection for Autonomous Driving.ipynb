{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection for Autonomous Driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<video width=\"800\" height=\"400\" src=\"nb_images/Result_BC.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<caption><center> Vedio 1: Result of this \"Object Detection for Autonomous Driving\" project. The original video was taken by Chaobin Yang from iphone hold on Boston College shuttle bus while driving around Boston College. \n",
    "</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib.pyplot import imshow\n",
    "import cv2\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes\n",
    "from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is provided by [drive.ai](https://www.drive.ai/). Images were gathered from cameras mounted to the front of cars. Use YOLO algorithm to recognize objects in images. Recognized objects are labelled with a square box. In the notebook, I did following:\n",
    "- F\n",
    "- max\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a box\n",
    "$b_x$ and $b_y$ define center of box and $b_h$ and $b_w$ define size of box. If there are 80 categories to recognize, I can either represent the category of object by:\n",
    "- $i)$ label c as an integer from 1 to 80: 6 elements to represent a box\n",
    "- $ii)$ one hot vector whose $c_{th}$ place is 1 and all others are 0s: 85 elements to represent a box\n",
    "\n",
    "<img src=\"nb_images/box_label.png\" style=\"width:500px;height:250;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO (\"you only look once\") requires only one forward propagation pass through the network to make predictions. Thus it \"only looks once\" at the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model details\n",
    "\n",
    "- The **input** is m images in tensor of shape (m, 608, 608, 3)\n",
    "- The **output** is a list of boxes along with the recognized classes (m, 19, 19, 5, 85). Each image is cut into 19*19 cells. Each cell has five boxes. Each bounding box is represented by 6 numbers $(p_c, b_x, b_y, b_h, b_w, c)$ as explained above. If $c$ is expanded into an 80-dimensional vector, each bounding box is then represented by 85 numbers. \n",
    "\n",
    "If the center/midpoint of an object falls into a grid cell, that grid cell is responsible for detecting that object. A cell can have maximum of 5 objects centered inside.\n",
    "\n",
    "YOLO architecture: IMAGE (m, 608, 608, 3) -> DEEP CNN -> ENCODING (m, 19, 19, 5, 85).\n",
    "\n",
    "<img src=\"nb_images/architecture.png\" style=\"width:700px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Filtering boxes with class scores\n",
    "\n",
    "Each cell gives 5 boxes. So the model can predict 19x19x5=1805 boxes by just looking once at the image. So we need\n",
    "- First, only keep boxes with high class score (more confident about detecting an object)\n",
    "- Second, only keep one box when several overlapping boxes are detecting the same object\n",
    "<img src=\"nb_images/anchor_map.png\" style=\"width:200px;height:200;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**yolo_filter_boxes( box_confidence, boxes, box_class_probs, threshold)** will filter boxes:\n",
    "\n",
    "Step 1:\n",
    "Scores of every class are calculated by $p_c$ * ($c_1$, $c_2$, ..., $c_{79}$, $c_{80}$)\n",
    "- \"box_confidence\" is $p_c$, a tensor of shape (19, 19, 5, 1)\n",
    "- \"box_class_probs\" is ( $c_1$, $c_2$, ..., $c_{79}$, $c_{80}$), a tensor of shape (19, 19, 5, 80)\n",
    "- \"boxes\" is sizes of all the boxes, containing $(b_x, b_y, b_h, b_w)$, a tensor of shape (19, 19, 5, 4)\n",
    "\n",
    "Step 2:\n",
    "In every box, find the index and value of class with max score. Index is saved as \"box_classes\" and value is saved as \"box_class_scores\". Create a filtering mask based on \"box_class_scores\" by using \"threshold\".\n",
    "\n",
    "Step 3:\n",
    "Apply filtering mask to all boxes and got boxes with scores higher than threshold.\n",
    "- \"scores\" -- tensor of shape (number_selected_boxes, 1), containing the class probability score for selected boxes\n",
    "- \"boxes\" -- tensor of shape (number_selected_boxes, 4), containing $(b_x, b_y, b_h, b_w)$ coordinates of selected boxes\n",
    "- \"classes\" -- tensor of shape (number_selected_boxes, 1), containing the index of the class detected by the selected boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):\n",
    "    # Step 1: Compute box scores.\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    \n",
    "    # Step 2: find the index and value of class with max score.\n",
    "    box_classes = K.argmax(box_scores, axis=-1)\n",
    "    box_class_scores = K.max(box_scores, axis=-1, keepdims=False)\n",
    "    # Create a filtering mask based on \"box_class_scores\" by using \"threshold\". The mask have the\n",
    "    # same dimension as box_class_scores, and be True for the boxes you want to keep \n",
    "    filtering_mask = box_class_scores>=threshold\n",
    "    \n",
    "    # Step 3: Apply the mask to scores, boxes and classes, select box with score higher than threshold\n",
    "    scores = tf.boolean_mask(box_class_scores, filtering_mask)\n",
    "    boxes = tf.boolean_mask(boxes, filtering_mask)\n",
    "    classes = tf.boolean_mask(box_classes, filtering_mask)\n",
    "    \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Non-max suppression ###\n",
    "\n",
    "After filtering by thresholding over the classes scores, I end up a lot of overlapping boxes. A second filter for selecting the right boxes is called non-maximum suppression (NMS). \n",
    "<img src=\"nb_images/non-max-suppression.png\" style=\"width:500px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-max suppression uses the very important function called **\"Intersection over Union\"**, or IoU.\n",
    "<img src=\"nb_images/iou.png\" style=\"width:500px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iou(box1, box2)** calculates IoU shown above. **IoU large->more overlap->delete**. (x1, y1, x2, y2) is upper left and lower right in box.  \n",
    "box1 -- first box, list object with coordinates (x1, y1, x2, y2)  \n",
    "box2 -- second box, list object with coordinates (x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    \n",
    "    # Calculate its INTER Area.\n",
    "    xi1 = max(box1[0],box2[0])\n",
    "    yi1 = max(box1[1],box2[1])\n",
    "    xi2 = min(box1[2],box2[2])\n",
    "    yi2 = min(box1[3],box2[3])\n",
    "    inter_area = (xi2-xi1)*(yi2-yi1)\n",
    "\n",
    "    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
    "    box1_area = (box1[3] - box1[1]) * (box1[2] - box1[0])\n",
    "    box2_area = (box2[3] - box2[1]) * (box2[2] - box2[0])\n",
    "    union_area = box1_area+box2_area-inter_area\n",
    "    \n",
    "    # compute the IoU\n",
    "    iou = inter_area/union_area\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5)**   \n",
    "Implement non-max suppression. The key steps are:  \n",
    "1. Select the box that has the highest score.\n",
    "2. Compute its overlap with all other boxes, and remove boxes that overlap it more than \"iou_threshold\".\n",
    "3. Go back to step 1 and iterate until the selected box has lowest score among all boxes\n",
    "\n",
    "Arguments:  \n",
    "\"scores\",\"boxes\",\"classes\" are output of yolo_filter_boxes(). \"max_boxes\": maximum number of predicted boxes you'd like\n",
    "\n",
    "Returns:  \n",
    "\"scores\",\"boxes\",\"classes\" selected boxes after non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
    "    \n",
    "    # tensor to be used in tf.image.non_max_suppression()\n",
    "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')\n",
    "    \n",
    "    # initialize variable max_boxes_tensor\n",
    "    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) \n",
    "    \n",
    "    # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep\n",
    "    nms_indices = tf.image.non_max_suppression(boxes,scores,max_boxes, iou_threshold,name=None)\n",
    "    \n",
    "    # Use K.gather() to select only nms_indices from scores, boxes and classes\n",
    "    scores = K.gather(scores,nms_indices)\n",
    "    boxes = K.gather(boxes,nms_indices)\n",
    "    classes = K.gather(classes,nms_indices)\n",
    "    \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Wrapping up these two filtering\n",
    "\n",
    "**yolo_eval(yolo_outputs, image_shape, max_boxes, score_threshold, iou_threshold)** use the output of the deep CNN (the 19x19x5x85 dimensional encoding) and filtering through all the boxes using the functions just implemented.  \n",
    "\n",
    "Arguments:  \n",
    "\"yolo_outputs\" -- output of the deep CNN model (for image_shape of (608, 608, 3)), contains 4 tensors:  \n",
    "                    box_confidence: tensor of shape (None, 19, 19, 5, 1)\n",
    "                    box_xy: tensor of shape (None, 19, 19, 5, 2)\n",
    "                    box_wh: tensor of shape (None, 19, 19, 5, 2)\n",
    "                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)\n",
    "\"image_shape\" -- tensor of shape (2,) containing the input shape  \n",
    "\n",
    "Returns:  \n",
    "\"scores\", \"boxes\", \"classes\" are all output of yolo_non_max_suppression\n",
    "\n",
    "**Note:boxes = scale_boxes(boxes, image_shape)** rescales the boxes:  \n",
    "YOLO's network was trained to run on 608x608 images. To test this data on a different size--for example, 720x1280 images--need to rescale the boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5):\n",
    "    \n",
    "    # Retrieve outputs of the YOLO model \n",
    "    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs\n",
    "\n",
    "    # Convert boxes to be ready for filtering functions \n",
    "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
    "\n",
    "    # perform Score-filtering with a threshold of score_threshold\n",
    "    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold)\n",
    "    \n",
    "    # Scale boxes back to original image shape\n",
    "    boxes = scale_boxes(boxes, image_shape)\n",
    "\n",
    "    # perform Non-max suppression with a threshold of iou_threshold \n",
    "    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold)\n",
    "\n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Test YOLO pretrained model with image\n",
    "Create a session and start to test on graph\n",
    "- Start a session\n",
    "- Define classes, anchors and image shape. 80 classes and 5 anchors are loaded from files. image_shape need match test image.\n",
    "- Load a pretrained model from \"yolo.h5\" and the summary of model. Model converts input images (shape: (m, 608, 608, 3)) into a tensor of shape (m, 19, 19, 5, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 608, 608, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 608, 608, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 608, 608, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 608, 608, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 304, 304, 32) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 304, 304, 64) 18432       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 304, 304, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 304, 304, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 152, 152, 64) 0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 152, 152, 128 73728       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 152, 152, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 152, 152, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 152, 152, 64) 8192        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 152, 152, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 152, 152, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 152, 152, 128 73728       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 152, 152, 128 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 152, 152, 128 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 76, 76, 128)  0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 76, 76, 256)  294912      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 76, 76, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 76, 76, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 76, 76, 128)  32768       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 76, 76, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 76, 76, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 76, 76, 256)  294912      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 76, 76, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 76, 76, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 38, 38, 256)  0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 38, 38, 512)  1179648     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 38, 38, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 38, 38, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 38, 38, 256)  131072      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 38, 38, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 38, 38, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 38, 38, 512)  1179648     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 38, 38, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 38, 38, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 38, 38, 256)  131072      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 38, 38, 256)  1024        conv2d_12[0][0]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 38, 38, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 38, 38, 512)  1179648     leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 38, 38, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 38, 38, 512)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 19, 19, 512)  0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 19, 19, 1024) 4718592     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 19, 19, 1024) 4096        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 19, 19, 512)  524288      leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 19, 19, 512)  2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 19, 19, 512)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 19, 19, 1024) 4718592     leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 19, 19, 1024) 4096        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 19, 19, 512)  524288      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 19, 19, 512)  2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 19, 19, 512)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 19, 19, 1024) 4718592     leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 19, 19, 1024) 4096        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 19, 19, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 19, 19, 1024) 4096        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 38, 38, 64)   32768       leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 38, 38, 64)   256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 19, 19, 1024) 9437184     leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 38, 38, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 19, 19, 1024) 4096        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "space_to_depth_x2 (Lambda)      (None, 19, 19, 256)  0           leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 19, 19, 1280) 0           space_to_depth_x2[0][0]          \n",
      "                                                                 leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 19, 19, 1024) 11796480    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 19, 19, 1024) 4096        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 19, 19, 425)  435625      leaky_re_lu_22[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 50,983,561\n",
      "Trainable params: 50,962,889\n",
      "Non-trainable params: 20,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#start a session\n",
    "sess = K.get_session()\n",
    "\n",
    "# deine classes, anchors and image shape\n",
    "class_names = read_classes(\"model_data/coco_classes.txt\")\n",
    "anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
    "image_shape = (1080., 1920.)\n",
    "\n",
    "#load pretrained model\n",
    "yolo_model = load_model(\"model_data/yolo.h5\")\n",
    "#model summary\n",
    "yolo_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))\n",
    "\n",
    "scores, boxes, classes = yolo_eval(yolo_outputs, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sess, image_file):\n",
    "    \"\"\"\n",
    "    Runs the graph stored in \"sess\" to predict boxes for \"image_file\". Prints and plots the preditions.\n",
    "    \n",
    "    Arguments:\n",
    "    sess -- your tensorflow/Keras session containing the YOLO graph\n",
    "    image_file -- name of an image stored in the \"images\" folder.\n",
    "    \n",
    "    Returns:\n",
    "    out_scores -- tensor of shape (None, ), scores of the predicted boxes\n",
    "    out_boxes -- tensor of shape (None, 4), coordinates of the predicted boxes\n",
    "    out_classes -- tensor of shape (None, ), class index of the predicted boxes\n",
    "    \n",
    "    Note: \"None\" actually represents the number of predicted boxes, it varies between 0 and max_boxes. \n",
    "    \"\"\"\n",
    "\n",
    "    # Preprocess your image\n",
    "    image, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (608, 608))\n",
    "\n",
    "    # Run the session with the correct tensors and choose the correct placeholders in the feed_dict.\n",
    "    # You'll need to use feed_dict={yolo_model.input: ... , K.learning_phase(): 0})\n",
    "\n",
    "    out_scores, out_boxes, out_classes = sess.run(yolo_eval(yolo_outputs, image_shape),feed_dict={yolo_model.input: image_data, K.learning_phase(): 0})\n",
    "\n",
    "    # Print predictions info\n",
    "    print('Found {} boxes for {}'.format(len(out_boxes), image_file))\n",
    "    # Generate colors for drawing bounding boxes.\n",
    "    colors = generate_colors(class_names)\n",
    "    # Draw bounding boxes on the image file\n",
    "    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)\n",
    "    # Save the predicted bounding box on the image\n",
    "    image.save(os.path.join(\"out\", image_file), quality=90)\n",
    "    # Display the results in the notebook\n",
    "    output_image = scipy.misc.imread(os.path.join(\"out\", image_file))\n",
    "    \n",
    "    #imshow(output_image)\n",
    "    \n",
    "    return out_scores, out_boxes, out_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<img src=\"images/test.jpg\" style=\"width:640px;height:360;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 boxes for test.jpg\n",
      "car 0.61 (915, 517) (1066, 611)\n",
      "stop sign 0.62 (1507, 428) (1555, 465)\n",
      "car 0.62 (960, 509) (1093, 594)\n",
      "car 0.65 (630, 553) (886, 678)\n",
      "bus 0.81 (17, 363) (568, 935)\n"
     ]
    }
   ],
   "source": [
    "out_scores, out_boxes, out_classes = predict(sess, \"test.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the output image we just predicted. Five boxes including a bus, three cars and a stop signal are detected\n",
    "<img src=\"nb_images/test.jpg\" style=\"width:640px;height:360;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Video Visualization\n",
    "\n",
    "Now I got a Video and test this YOLO model with it. \n",
    "\n",
    "<center>\n",
    "<video width=\"640\" height=\"360\" src=\"images/BC.mov\" type=\"video/mov\" controls>\n",
    "</video>\n",
    "    \n",
    "</center>\n",
    "<caption><center> Vedio 2: Original video taken by Chaobin Yang from iphone hold on Boston College shuttle bus while driving around Boston College. \n",
    "</center></caption>\n",
    "\n",
    "### 4.1 break video to many images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('images/BC.mov')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "    cv2.imwrite(\"images/BC%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "    success,image = vidcap.read()\n",
    "    count += 1\n",
    "    if count>1197: break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Predict every image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 boxes for BC0.jpg\n",
      "truck 0.61 (1729, 282) (1899, 581)\n",
      "car 0.70 (446, 466) (605, 573)\n",
      "car 0.79 (232, 492) (409, 559)\n",
      "Found 3 boxes for BC1.jpg\n",
      "truck 0.62 (1728, 282) (1900, 581)\n",
      "car 0.67 (446, 466) (608, 573)\n",
      "car 0.79 (231, 491) (411, 560)\n",
      "Found 3 boxes for BC2.jpg\n",
      "car 0.64 (443, 466) (607, 570)\n",
      "truck 0.67 (1729, 285) (1900, 578)\n",
      "car 0.78 (229, 492) (408, 559)\n",
      "Found 3 boxes for BC3.jpg\n",
      "car 0.64 (442, 466) (604, 569)\n",
      "truck 0.64 (1727, 286) (1900, 577)\n",
      "car 0.77 (229, 492) (410, 558)\n",
      "Found 2 boxes for BC4.jpg\n",
      "car 0.60 (443, 466) (607, 569)\n",
      "car 0.76 (229, 492) (409, 558)\n",
      "Found 3 boxes for BC5.jpg\n",
      "truck 0.60 (1727, 284) (1899, 572)\n",
      "car 0.61 (434, 473) (569, 568)\n",
      "car 0.77 (230, 491) (412, 559)\n",
      "Found 1 boxes for BC6.jpg\n",
      "car 0.77 (229, 492) (411, 557)\n",
      "Found 1 boxes for BC7.jpg\n",
      "car 0.77 (229, 490) (408, 558)\n",
      "Found 2 boxes for BC8.jpg\n",
      "car 0.63 (430, 466) (636, 573)\n",
      "car 0.78 (227, 491) (408, 558)\n",
      "Found 2 boxes for BC9.jpg\n",
      "car 0.65 (425, 466) (640, 575)\n",
      "car 0.78 (228, 490) (409, 557)\n",
      "Found 3 boxes for BC10.jpg\n",
      "car 0.66 (1570, 407) (1701, 513)\n",
      "car 0.69 (424, 465) (634, 573)\n",
      "car 0.79 (226, 490) (408, 557)\n",
      "Found 3 boxes for BC11.jpg\n",
      "car 0.64 (1572, 408) (1699, 513)\n",
      "car 0.69 (420, 463) (635, 573)\n",
      "car 0.80 (225, 489) (406, 558)\n",
      "Found 3 boxes for BC12.jpg\n",
      "car 0.62 (1573, 408) (1702, 514)\n",
      "car 0.65 (423, 465) (629, 572)\n",
      "car 0.79 (228, 490) (405, 556)\n",
      "Found 2 boxes for BC13.jpg\n",
      "car 0.65 (415, 470) (576, 574)\n",
      "car 0.78 (230, 489) (407, 556)\n",
      "Found 3 boxes for BC14.jpg\n",
      "car 0.62 (1580, 411) (1716, 516)\n",
      "car 0.67 (422, 466) (626, 574)\n",
      "car 0.78 (227, 489) (404, 556)\n",
      "Found 2 boxes for BC15.jpg\n",
      "car 0.68 (410, 467) (582, 574)\n",
      "car 0.79 (226, 488) (401, 556)\n",
      "Found 3 boxes for BC16.jpg\n",
      "car 0.61 (1567, 394) (1712, 497)\n",
      "car 0.69 (407, 465) (584, 575)\n",
      "car 0.80 (227, 488) (400, 558)\n",
      "Found 3 boxes for BC17.jpg\n",
      "car 0.63 (1566, 396) (1710, 498)\n",
      "car 0.72 (405, 464) (582, 575)\n",
      "car 0.81 (229, 486) (396, 558)\n",
      "Found 3 boxes for BC18.jpg\n",
      "car 0.62 (1568, 398) (1707, 497)\n",
      "car 0.70 (404, 463) (579, 575)\n",
      "car 0.81 (230, 485) (392, 557)\n",
      "Found 3 boxes for BC19.jpg\n",
      "car 0.69 (404, 463) (579, 576)\n",
      "car 0.70 (1580, 409) (1718, 511)\n",
      "car 0.81 (230, 485) (391, 557)\n",
      "Found 3 boxes for BC20.jpg\n",
      "car 0.71 (1581, 412) (1720, 509)\n",
      "car 0.71 (399, 461) (581, 574)\n",
      "car 0.82 (228, 483) (392, 556)\n",
      "Found 3 boxes for BC21.jpg\n",
      "car 0.71 (393, 461) (578, 574)\n",
      "car 0.72 (1584, 411) (1727, 510)\n",
      "car 0.80 (229, 483) (398, 556)\n",
      "Found 3 boxes for BC22.jpg\n",
      "car 0.71 (1587, 412) (1732, 510)\n",
      "car 0.73 (386, 460) (576, 573)\n",
      "car 0.81 (229, 482) (396, 555)\n",
      "Found 3 boxes for BC23.jpg\n",
      "car 0.73 (384, 460) (577, 573)\n",
      "car 0.74 (1590, 413) (1728, 509)\n",
      "car 0.81 (229, 481) (397, 556)\n",
      "Found 3 boxes for BC24.jpg\n",
      "car 0.73 (378, 461) (576, 575)\n",
      "car 0.79 (1592, 412) (1726, 508)\n",
      "car 0.79 (232, 479) (397, 555)\n",
      "Found 4 boxes for BC25.jpg\n",
      "traffic light 0.60 (725, 1) (870, 97)\n",
      "car 0.74 (378, 461) (572, 575)\n",
      "car 0.78 (1595, 413) (1724, 508)\n",
      "car 0.79 (232, 479) (396, 554)\n",
      "Found 3 boxes for BC26.jpg\n",
      "car 0.74 (377, 461) (574, 575)\n",
      "car 0.79 (1597, 415) (1723, 507)\n",
      "car 0.80 (231, 478) (392, 554)\n",
      "Found 3 boxes for BC27.jpg\n",
      "car 0.75 (373, 460) (569, 575)\n",
      "car 0.78 (1599, 415) (1728, 507)\n",
      "car 0.79 (230, 477) (395, 556)\n",
      "Found 3 boxes for BC28.jpg\n",
      "car 0.74 (374, 460) (565, 574)\n",
      "car 0.78 (1599, 415) (1729, 508)\n",
      "car 0.79 (230, 477) (393, 555)\n",
      "Found 3 boxes for BC29.jpg\n",
      "car 0.72 (371, 462) (559, 573)\n",
      "car 0.76 (1604, 416) (1731, 508)\n",
      "car 0.77 (230, 477) (396, 555)\n",
      "Found 3 boxes for BC30.jpg\n",
      "car 0.73 (370, 461) (556, 573)\n",
      "car 0.75 (1600, 417) (1732, 510)\n",
      "car 0.76 (230, 476) (396, 556)\n",
      "Found 4 boxes for BC31.jpg\n",
      "traffic light 0.63 (758, 2) (872, 92)\n",
      "car 0.72 (229, 474) (396, 556)\n",
      "car 0.74 (377, 448) (587, 566)\n",
      "car 0.77 (1605, 415) (1737, 509)\n",
      "Found 3 boxes for BC32.jpg\n",
      "car 0.72 (243, 467) (389, 547)\n",
      "car 0.74 (377, 447) (584, 568)\n",
      "car 0.76 (1606, 416) (1733, 509)\n",
      "Found 4 boxes for BC33.jpg\n",
      "traffic light 0.60 (759, 1) (871, 94)\n",
      "car 0.74 (1612, 414) (1738, 511)\n",
      "car 0.75 (243, 467) (389, 548)\n",
      "car 0.75 (379, 448) (585, 567)\n",
      "Found 4 boxes for BC34.jpg\n",
      "traffic light 0.60 (758, 1) (872, 94)\n",
      "car 0.73 (379, 448) (586, 567)\n",
      "car 0.75 (1612, 415) (1738, 513)\n",
      "car 0.76 (242, 466) (389, 549)\n",
      "Found 3 boxes for BC35.jpg\n",
      "car 0.75 (243, 464) (390, 550)\n",
      "car 0.75 (376, 447) (583, 568)\n",
      "car 0.77 (1614, 414) (1737, 511)\n",
      "Found 3 boxes for BC36.jpg\n",
      "car 0.73 (243, 463) (393, 549)\n",
      "car 0.77 (1615, 412) (1741, 511)\n",
      "car 0.78 (371, 446) (586, 569)\n",
      "Found 3 boxes for BC37.jpg\n",
      "car 0.75 (243, 464) (391, 549)\n",
      "car 0.76 (1620, 412) (1745, 512)\n",
      "car 0.78 (369, 446) (582, 570)\n",
      "Found 4 boxes for BC38.jpg\n",
      "traffic light 0.67 (762, 2) (879, 96)\n",
      "car 0.76 (245, 464) (391, 548)\n",
      "car 0.76 (1624, 412) (1750, 513)\n",
      "car 0.78 (370, 447) (579, 568)\n",
      "Found 4 boxes for BC39.jpg\n",
      "traffic light 0.70 (764, 1) (880, 97)\n",
      "car 0.76 (1627, 413) (1750, 512)\n",
      "car 0.76 (243, 466) (397, 546)\n",
      "car 0.79 (369, 448) (584, 568)\n",
      "Found 4 boxes for BC40.jpg\n",
      "traffic light 0.67 (764, 2) (882, 95)\n",
      "car 0.77 (245, 467) (389, 545)\n",
      "car 0.77 (1624, 414) (1751, 513)\n",
      "car 0.78 (371, 447) (581, 568)\n",
      "Found 3 boxes for BC41.jpg\n",
      "car 0.77 (1625, 415) (1757, 513)\n",
      "car 0.77 (244, 467) (390, 544)\n",
      "car 0.80 (369, 447) (589, 569)\n",
      "Found 3 boxes for BC42.jpg\n",
      "car 0.77 (243, 468) (390, 543)\n",
      "car 0.79 (366, 447) (587, 568)\n",
      "car 0.81 (1626, 415) (1765, 512)\n",
      "Found 3 boxes for BC43.jpg\n",
      "car 0.78 (367, 448) (580, 568)\n",
      "car 0.78 (245, 470) (389, 542)\n",
      "car 0.80 (1631, 415) (1768, 510)\n",
      "Found 3 boxes for BC44.jpg\n",
      "car 0.76 (363, 448) (575, 567)\n",
      "car 0.79 (246, 470) (387, 542)\n",
      "car 0.81 (1633, 417) (1774, 508)\n",
      "Found 3 boxes for BC45.jpg\n",
      "car 0.76 (369, 448) (573, 568)\n",
      "car 0.80 (1635, 417) (1773, 508)\n",
      "car 0.81 (247, 469) (386, 543)\n",
      "Found 3 boxes for BC46.jpg\n",
      "car 0.76 (359, 449) (571, 566)\n",
      "car 0.79 (246, 469) (385, 541)\n",
      "car 0.81 (1642, 417) (1776, 507)\n",
      "Found 3 boxes for BC47.jpg\n",
      "car 0.76 (360, 448) (569, 566)\n",
      "car 0.80 (249, 469) (384, 539)\n",
      "car 0.83 (1646, 417) (1777, 508)\n",
      "Found 4 boxes for BC48.jpg\n",
      "traffic light 0.61 (784, 5) (906, 91)\n",
      "car 0.73 (363, 447) (561, 566)\n",
      "car 0.79 (249, 468) (381, 540)\n",
      "car 0.81 (1646, 420) (1779, 505)\n",
      "Found 4 boxes for BC49.jpg\n",
      "traffic light 0.62 (789, 4) (910, 92)\n",
      "car 0.73 (362, 446) (557, 568)\n",
      "car 0.76 (248, 467) (382, 539)\n",
      "car 0.84 (1645, 420) (1782, 506)\n",
      "Found 5 boxes for BC50.jpg\n",
      "car 0.62 (463, 440) (577, 553)\n",
      "traffic light 0.63 (787, 4) (913, 91)\n",
      "car 0.71 (363, 444) (551, 569)\n",
      "car 0.74 (249, 465) (386, 540)\n",
      "car 0.81 (1645, 422) (1782, 506)\n",
      "Found 4 boxes for BC51.jpg\n",
      "traffic light 0.66 (794, 4) (919, 92)\n",
      "car 0.73 (250, 465) (383, 540)\n",
      "car 0.77 (358, 444) (562, 570)\n",
      "car 0.82 (1645, 421) (1782, 508)\n",
      "Found 4 boxes for BC52.jpg\n",
      "traffic light 0.65 (798, 4) (922, 91)\n",
      "car 0.74 (250, 465) (387, 540)\n",
      "car 0.78 (360, 444) (566, 570)\n",
      "car 0.80 (1648, 422) (1779, 506)\n",
      "Found 4 boxes for BC53.jpg\n",
      "traffic light 0.65 (803, 3) (923, 92)\n",
      "car 0.75 (250, 466) (386, 539)\n",
      "car 0.77 (1650, 422) (1778, 508)\n",
      "car 0.77 (358, 446) (566, 567)\n",
      "Found 4 boxes for BC54.jpg\n",
      "traffic light 0.61 (799, 3) (927, 89)\n",
      "car 0.71 (1650, 422) (1778, 507)\n",
      "car 0.75 (249, 463) (385, 539)\n",
      "car 0.76 (354, 444) (563, 567)\n",
      "Found 3 boxes for BC55.jpg\n",
      "car 0.75 (248, 463) (388, 540)\n",
      "car 0.75 (1649, 422) (1778, 506)\n",
      "car 0.77 (348, 444) (566, 565)\n",
      "Found 3 boxes for BC56.jpg\n",
      "car 0.66 (1662, 419) (1791, 529)\n",
      "car 0.74 (248, 463) (389, 540)\n",
      "car 0.78 (347, 441) (566, 569)\n",
      "Found 3 boxes for BC57.jpg\n",
      "car 0.73 (247, 464) (390, 540)\n",
      "car 0.77 (1657, 421) (1790, 523)\n",
      "car 0.79 (344, 441) (568, 570)\n",
      "Found 3 boxes for BC58.jpg\n",
      "car 0.73 (249, 466) (389, 539)\n",
      "car 0.77 (350, 441) (560, 571)\n",
      "car 0.78 (1658, 420) (1792, 521)\n",
      "Found 3 boxes for BC59.jpg\n",
      "car 0.70 (249, 465) (390, 539)\n",
      "car 0.77 (346, 440) (562, 571)\n",
      "car 0.82 (1653, 421) (1796, 518)\n",
      "Found 3 boxes for BC60.jpg\n",
      "car 0.66 (248, 464) (399, 539)\n",
      "car 0.81 (335, 440) (571, 571)\n",
      "car 0.81 (1653, 419) (1799, 519)\n",
      "Found 3 boxes for BC61.jpg\n",
      "car 0.63 (249, 463) (399, 540)\n",
      "car 0.83 (1653, 419) (1799, 520)\n",
      "car 0.84 (326, 438) (571, 573)\n",
      "Found 3 boxes for BC62.jpg\n",
      "car 0.64 (247, 463) (393, 542)\n",
      "car 0.77 (1657, 421) (1800, 523)\n",
      "car 0.83 (326, 436) (569, 574)\n",
      "Found 3 boxes for BC63.jpg\n",
      "car 0.60 (246, 463) (396, 543)\n",
      "car 0.78 (1656, 421) (1799, 522)\n",
      "car 0.84 (313, 437) (566, 576)\n",
      "Found 2 boxes for BC64.jpg\n",
      "car 0.79 (1656, 420) (1798, 521)\n",
      "car 0.85 (309, 434) (568, 579)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 boxes for BC65.jpg\n",
      "car 0.78 (1657, 419) (1799, 521)\n",
      "car 0.85 (297, 435) (567, 579)\n",
      "Found 2 boxes for BC66.jpg\n",
      "car 0.80 (1656, 417) (1798, 520)\n",
      "car 0.83 (294, 433) (568, 579)\n",
      "Found 2 boxes for BC67.jpg\n",
      "car 0.81 (1656, 415) (1798, 521)\n",
      "car 0.83 (302, 435) (565, 575)\n",
      "Found 2 boxes for BC68.jpg\n",
      "car 0.82 (1656, 415) (1797, 521)\n",
      "car 0.83 (300, 438) (560, 573)\n",
      "Found 2 boxes for BC69.jpg\n",
      "car 0.80 (309, 436) (555, 574)\n",
      "car 0.83 (1654, 415) (1796, 520)\n",
      "Found 2 boxes for BC70.jpg\n",
      "car 0.80 (301, 437) (548, 575)\n",
      "car 0.83 (1654, 412) (1798, 519)\n",
      "Found 2 boxes for BC71.jpg\n",
      "car 0.79 (300, 438) (546, 576)\n",
      "car 0.83 (1654, 411) (1795, 519)\n",
      "Found 2 boxes for BC72.jpg\n",
      "car 0.81 (298, 436) (546, 579)\n",
      "car 0.83 (1654, 411) (1795, 518)\n",
      "Found 2 boxes for BC73.jpg\n",
      "car 0.78 (302, 436) (542, 580)\n",
      "car 0.84 (1652, 411) (1794, 519)\n",
      "Found 2 boxes for BC74.jpg\n",
      "car 0.73 (306, 434) (537, 580)\n",
      "car 0.84 (1650, 411) (1795, 519)\n",
      "Found 3 boxes for BC75.jpg\n",
      "traffic light 0.62 (805, 6) (927, 93)\n",
      "car 0.69 (248, 452) (494, 577)\n",
      "car 0.82 (1652, 411) (1794, 518)\n",
      "Found 3 boxes for BC76.jpg\n",
      "traffic light 0.64 (804, 6) (925, 93)\n",
      "car 0.71 (241, 452) (495, 577)\n",
      "car 0.78 (1654, 412) (1791, 519)\n",
      "Found 3 boxes for BC77.jpg\n",
      "traffic light 0.65 (807, 5) (926, 96)\n",
      "car 0.71 (242, 450) (496, 579)\n",
      "car 0.79 (1652, 414) (1792, 518)\n",
      "Found 3 boxes for BC78.jpg\n",
      "traffic light 0.66 (806, 5) (925, 95)\n",
      "car 0.74 (242, 447) (494, 583)\n",
      "car 0.82 (1650, 414) (1791, 517)\n",
      "Found 3 boxes for BC79.jpg\n",
      "traffic light 0.65 (803, 5) (922, 93)\n",
      "car 0.76 (232, 447) (492, 584)\n",
      "car 0.83 (1649, 411) (1793, 517)\n",
      "Found 4 boxes for BC80.jpg\n",
      "traffic light 0.62 (1004, 108) (1064, 250)\n",
      "traffic light 0.64 (797, 5) (921, 93)\n",
      "car 0.79 (231, 444) (492, 586)\n",
      "car 0.82 (1649, 408) (1790, 517)\n",
      "Found 4 boxes for BC81.jpg\n",
      "traffic light 0.63 (999, 108) (1064, 251)\n",
      "traffic light 0.67 (794, 4) (916, 93)\n",
      "car 0.76 (225, 448) (494, 584)\n",
      "car 0.81 (1651, 408) (1789, 518)\n",
      "Found 4 boxes for BC82.jpg\n",
      "traffic light 0.61 (997, 109) (1065, 244)\n",
      "traffic light 0.66 (790, 6) (911, 95)\n",
      "car 0.75 (228, 450) (487, 588)\n",
      "car 0.79 (1653, 408) (1789, 518)\n",
      "Found 3 boxes for BC83.jpg\n",
      "traffic light 0.66 (997, 108) (1064, 248)\n",
      "car 0.75 (1654, 408) (1789, 518)\n",
      "car 0.77 (222, 448) (487, 591)\n",
      "Found 4 boxes for BC84.jpg\n",
      "traffic light 0.61 (784, 8) (904, 96)\n",
      "traffic light 0.62 (993, 110) (1058, 241)\n",
      "car 0.63 (1656, 409) (1793, 519)\n",
      "car 0.79 (211, 446) (490, 593)\n",
      "Found 3 boxes for BC85.jpg\n",
      "traffic light 0.62 (783, 7) (904, 97)\n",
      "traffic light 0.69 (991, 109) (1053, 242)\n",
      "car 0.79 (207, 446) (490, 593)\n",
      "Found 3 boxes for BC86.jpg\n",
      "traffic light 0.60 (781, 7) (899, 96)\n",
      "traffic light 0.69 (988, 108) (1050, 244)\n",
      "car 0.81 (192, 444) (490, 592)\n",
      "Found 1 boxes for BC87.jpg\n",
      "car 0.82 (184, 444) (478, 592)\n",
      "Found 1 boxes for BC88.jpg\n",
      "car 0.82 (172, 448) (477, 592)\n",
      "Found 2 boxes for BC89.jpg\n",
      "car 0.61 (1657, 392) (1786, 497)\n",
      "car 0.79 (178, 446) (474, 590)\n",
      "Found 2 boxes for BC90.jpg\n",
      "traffic light 0.60 (968, 95) (1040, 210)\n",
      "car 0.82 (164, 446) (472, 591)\n",
      "Found 1 boxes for BC91.jpg\n",
      "car 0.83 (164, 448) (471, 590)\n",
      "Found 3 boxes for BC92.jpg\n",
      "car 0.61 (1632, 390) (1787, 489)\n",
      "traffic light 0.67 (969, 89) (1038, 207)\n",
      "car 0.82 (164, 449) (466, 591)\n",
      "Found 3 boxes for BC93.jpg\n",
      "traffic light 0.61 (969, 89) (1036, 209)\n",
      "car 0.62 (1632, 392) (1786, 491)\n",
      "car 0.81 (166, 445) (465, 590)\n",
      "Found 3 boxes for BC94.jpg\n",
      "traffic light 0.65 (774, 4) (888, 98)\n",
      "car 0.65 (1631, 392) (1787, 493)\n",
      "car 0.77 (174, 446) (458, 588)\n",
      "Found 3 boxes for BC95.jpg\n",
      "car 0.65 (1631, 391) (1787, 491)\n",
      "traffic light 0.66 (774, 1) (884, 100)\n",
      "car 0.72 (185, 445) (453, 587)\n",
      "Found 4 boxes for BC96.jpg\n",
      "traffic light 0.61 (960, 84) (1019, 215)\n",
      "car 0.66 (1633, 393) (1783, 493)\n",
      "traffic light 0.66 (773, 0) (884, 98)\n",
      "car 0.72 (137, 449) (409, 593)\n",
      "Found 3 boxes for BC97.jpg\n",
      "traffic light 0.63 (767, 2) (882, 93)\n",
      "car 0.64 (1629, 392) (1785, 493)\n",
      "car 0.76 (127, 450) (391, 596)\n",
      "Found 3 boxes for BC98.jpg\n",
      "traffic light 0.65 (765, 1) (877, 94)\n",
      "car 0.69 (1629, 393) (1781, 495)\n",
      "car 0.77 (114, 448) (397, 595)\n",
      "Found 2 boxes for BC99.jpg\n",
      "car 0.69 (1630, 394) (1776, 494)\n",
      "car 0.77 (108, 448) (400, 594)\n"
     ]
    }
   ],
   "source": [
    "images=list()\n",
    "#for i in range(1198):\n",
    "for i in range(30):\n",
    "    images.append(\"BC\"+str(i)+\".jpg\")\n",
    "    out_scores, out_boxes, out_classes = predict(sess, \"BC\"+str(i)+\".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Merge predicted images to one video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video Result_BC.mp4\n",
      "[MoviePy] Writing video Result_BC.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 72/72 [00:01<00:00, 38.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: Result_BC.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "# get every images for video\n",
    "clips = [ImageClip(\"out\\\\\"+ m).set_duration(0.03) for m in images]\n",
    "\n",
    "#concatenate images to a video\n",
    "concat_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "\n",
    "# output video to disk\n",
    "concat_clip.write_videofile(\"Result_BC.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output video \"Result_BC.mp4\" is vedeo 1 we shown in the beginning  \n",
    "\n",
    "<center>\n",
    "<video width=\"640\" height=\"360\" src=\"nb_images/Result_BC.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Reference\n",
    "\n",
    "[1] d  \n",
    "[2] d  \n",
    "[3] d  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
