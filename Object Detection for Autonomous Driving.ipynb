{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection for Autonomous Driving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<video width=\"800\" height=\"400\" src=\"nb_images/Result_BC.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "<caption><center> Vedio 1: Result of this \"Object Detection for Autonomous Driving\" project. The original video was taken by Chaobin Yang from iphone hold on Boston College shuttle bus while driving around Boston College. \n",
    "</center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from matplotlib.pyplot import imshow\n",
    "import cv2\n",
    "import scipy.io\n",
    "import scipy.misc\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from yolo_utils import read_classes, read_anchors, generate_colors, preprocess_image, draw_boxes, scale_boxes\n",
    "from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is provided by [drive.ai](https://www.drive.ai/). Images were gathered from cameras mounted to the front of cars. Use YOLO algorithm to recognize objects in images. Recognized objects are labelled with a square box. In the notebook, I did following:\n",
    "- Load pretrained model provided by [official YOLO website] (https://pjreddie.com/darknet/yolo/) . This model has 23 Conv2D layers and around 51 M parameters. Highly computationally expensive to train. \n",
    "- Define **yolo_filter_boxes** function to filter boxes. Boxes with high score/probability will be left \n",
    "- Define **yolo_non_max_suppression** function to eliminate overlapping boxes\n",
    "- Define **predict** fuction to make predict/object detection. Also draw sqaure boxes on images to show detected objects.\n",
    "- Label all detected objects in a video with squares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of a box\n",
    "$b_x$ and $b_y$ define center of box and $b_h$ and $b_w$ define size of box. If there are 80 categories to recognize, I can either represent the category of object by:\n",
    "- $i)$ label c as an integer from 1 to 80: 6 elements to represent a box\n",
    "- $ii)$ one hot vector whose $c_{th}$ place is 1 and all others are 0s: 85 elements to represent a box\n",
    "\n",
    "<img src=\"nb_images/box_label.png\" style=\"width:500px;height:250;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOLO (\"you only look once\") requires only one forward propagation pass through the network to make predictions. Thus it \"only looks once\" at the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model details\n",
    "\n",
    "- The **input** is m images in tensor of shape (m, 608, 608, 3)\n",
    "- The **output** is a list of boxes along with the recognized classes (m, 19, 19, 5, 85). Each image is cut into 19*19 cells. Each cell has five boxes. Each bounding box is represented by 6 numbers $(p_c, b_x, b_y, b_h, b_w, c)$ as explained above. If $c$ is expanded into an 80-dimensional vector, each bounding box is then represented by 85 numbers. \n",
    "\n",
    "If the center/midpoint of an object falls into a grid cell, that grid cell is responsible for detecting that object. A cell can have maximum of 5 objects centered inside.\n",
    "\n",
    "YOLO architecture: IMAGE (m, 608, 608, 3) -> DEEP CNN -> ENCODING (m, 19, 19, 5, 85).\n",
    "\n",
    "<img src=\"nb_images/architecture.png\" style=\"width:700px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Filtering boxes with class scores\n",
    "\n",
    "Each cell gives 5 boxes. So the model can predict 19x19x5=1805 boxes by just looking once at the image. So we need\n",
    "- First, only keep boxes with high class score (more confident about detecting an object)\n",
    "- Second, only keep one box when several overlapping boxes are detecting the same object\n",
    "\n",
    "<img src=\"nb_images/anchor_map.png\" style=\"width:200px;height:200;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**yolo_filter_boxes( box_confidence, boxes, box_class_probs, threshold)** will filter boxes:\n",
    "\n",
    "Step 1:\n",
    "Scores of every class are calculated by $p_c$ * ($c_1$, $c_2$, ..., $c_{79}$, $c_{80}$)\n",
    "- \"box_confidence\" is $p_c$, a tensor of shape (19, 19, 5, 1)\n",
    "- \"box_class_probs\" is ( $c_1$, $c_2$, ..., $c_{79}$, $c_{80}$), a tensor of shape (19, 19, 5, 80)\n",
    "- \"boxes\" is sizes of all the boxes, containing $(b_x, b_y, b_h, b_w)$, a tensor of shape (19, 19, 5, 4)\n",
    "\n",
    "Step 2:\n",
    "In every box, find the index and value of class with max score. Index is saved as \"box_classes\" and value is saved as \"box_class_scores\". Create a filtering mask based on \"box_class_scores\" by using \"threshold\".\n",
    "\n",
    "Step 3:\n",
    "Apply filtering mask to all boxes and got boxes with scores higher than threshold.\n",
    "- \"scores\" -- tensor of shape (number_selected_boxes, 1), containing the class probability score for selected boxes\n",
    "- \"boxes\" -- tensor of shape (number_selected_boxes, 4), containing $(b_x, b_y, b_h, b_w)$ coordinates of selected boxes\n",
    "- \"classes\" -- tensor of shape (number_selected_boxes, 1), containing the index of the class detected by the selected boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_filter_boxes(box_confidence, boxes, box_class_probs, threshold = .6):\n",
    "    # Step 1: Compute box scores.\n",
    "    box_scores = box_confidence * box_class_probs\n",
    "    \n",
    "    # Step 2: find the index and value of class with max score.\n",
    "    box_classes = K.argmax(box_scores, axis=-1)\n",
    "    box_class_scores = K.max(box_scores, axis=-1, keepdims=False)\n",
    "    # Create a filtering mask based on \"box_class_scores\" by using \"threshold\". The mask have the\n",
    "    # same dimension as box_class_scores, and be True for the boxes you want to keep \n",
    "    filtering_mask = box_class_scores>=threshold\n",
    "    \n",
    "    # Step 3: Apply the mask to scores, boxes and classes, select box with score higher than threshold\n",
    "    scores = tf.boolean_mask(box_class_scores, filtering_mask)\n",
    "    boxes = tf.boolean_mask(boxes, filtering_mask)\n",
    "    classes = tf.boolean_mask(box_classes, filtering_mask)\n",
    "    \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Non-max suppression ###\n",
    "\n",
    "After filtering by thresholding over the classes scores, I end up a lot of overlapping boxes. A second filter for selecting the right boxes is called non-maximum suppression (NMS). \n",
    "<img src=\"nb_images/non-max-suppression.png\" style=\"width:500px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-max suppression uses the very important function called **\"Intersection over Union\"**, or IoU.\n",
    "<img src=\"nb_images/iou.png\" style=\"width:500px;height:400;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iou(box1, box2)** calculates IoU shown above. **IoU large->more overlap->delete**. (x1, y1, x2, y2) is upper left and lower right in box.  \n",
    "box1 -- first box, list object with coordinates (x1, y1, x2, y2)  \n",
    "box2 -- second box, list object with coordinates (x1, y1, x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    \n",
    "    # Calculate its INTER Area.\n",
    "    xi1 = max(box1[0],box2[0])\n",
    "    yi1 = max(box1[1],box2[1])\n",
    "    xi2 = min(box1[2],box2[2])\n",
    "    yi2 = min(box1[3],box2[3])\n",
    "    inter_area = (xi2-xi1)*(yi2-yi1)\n",
    "\n",
    "    # Calculate the Union area by using Formula: Union(A,B) = A + B - Inter(A,B)\n",
    "    box1_area = (box1[3] - box1[1]) * (box1[2] - box1[0])\n",
    "    box2_area = (box2[3] - box2[1]) * (box2[2] - box2[0])\n",
    "    union_area = box1_area+box2_area-inter_area\n",
    "    \n",
    "    # compute the IoU\n",
    "    iou = inter_area/union_area\n",
    "\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5)**   \n",
    "Implement non-max suppression. The key steps are:  \n",
    "1. Select the box that has the highest score.\n",
    "2. Compute its overlap with all other boxes, and remove boxes that overlap it more than \"iou_threshold\".\n",
    "3. Go back to step 1 and iterate until the selected box has lowest score among all boxes\n",
    "\n",
    "Arguments:  \n",
    "\"scores\",\"boxes\",\"classes\" are output of yolo_filter_boxes(). \"max_boxes\": maximum number of predicted boxes you'd like\n",
    "\n",
    "Returns:  \n",
    "\"scores\",\"boxes\",\"classes\" selected boxes after non_max_suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_non_max_suppression(scores, boxes, classes, max_boxes = 10, iou_threshold = 0.5):\n",
    "    \n",
    "    # tensor to be used in tf.image.non_max_suppression()\n",
    "    max_boxes_tensor = K.variable(max_boxes, dtype='int32')\n",
    "    \n",
    "    # initialize variable max_boxes_tensor\n",
    "    K.get_session().run(tf.variables_initializer([max_boxes_tensor])) \n",
    "    \n",
    "    # Use tf.image.non_max_suppression() to get the list of indices corresponding to boxes you keep\n",
    "    nms_indices = tf.image.non_max_suppression(boxes,scores,max_boxes, iou_threshold,name=None)\n",
    "    \n",
    "    # Use K.gather() to select only nms_indices from scores, boxes and classes\n",
    "    scores = K.gather(scores,nms_indices)\n",
    "    boxes = K.gather(boxes,nms_indices)\n",
    "    classes = K.gather(classes,nms_indices)\n",
    "    \n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Wrapping up these two filtering\n",
    "\n",
    "**yolo_eval(yolo_outputs, image_shape, max_boxes, score_threshold, iou_threshold)** use the output of the deep CNN (the 19x19x5x85 dimensional encoding) and filtering through all the boxes using the functions just implemented.  \n",
    "\n",
    "Arguments:  \n",
    "\"yolo_outputs\" -- output of the deep CNN model (for image_shape of (608, 608, 3)), contains 4 tensors:  \n",
    "                    box_confidence: tensor of shape (None, 19, 19, 5, 1)\n",
    "                    box_xy: tensor of shape (None, 19, 19, 5, 2)\n",
    "                    box_wh: tensor of shape (None, 19, 19, 5, 2)\n",
    "                    box_class_probs: tensor of shape (None, 19, 19, 5, 80)\n",
    "\"image_shape\" -- tensor of shape (2,) containing the input shape  \n",
    "\n",
    "Returns:  \n",
    "\"scores\", \"boxes\", \"classes\" are all output of yolo_non_max_suppression\n",
    "\n",
    "**Note:boxes = scale_boxes(boxes, image_shape)** rescales the boxes:  \n",
    "YOLO's network was trained to run on 608x608 images. To test this data on a different size--for example, 720x1280 images--need to rescale the boxes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_eval(yolo_outputs, image_shape = (720., 1280.), max_boxes=10, score_threshold=.6, iou_threshold=.5):\n",
    "    \n",
    "    # Retrieve outputs of the YOLO model \n",
    "    box_confidence, box_xy, box_wh, box_class_probs = yolo_outputs\n",
    "\n",
    "    # Convert boxes to be ready for filtering functions \n",
    "    boxes = yolo_boxes_to_corners(box_xy, box_wh)\n",
    "\n",
    "    # perform Score-filtering with a threshold of score_threshold\n",
    "    scores, boxes, classes = yolo_filter_boxes(box_confidence, boxes, box_class_probs, score_threshold)\n",
    "    \n",
    "    # Scale boxes back to original image shape\n",
    "    boxes = scale_boxes(boxes, image_shape)\n",
    "\n",
    "    # perform Non-max suppression with a threshold of iou_threshold \n",
    "    scores, boxes, classes = yolo_non_max_suppression(scores, boxes, classes, max_boxes, iou_threshold)\n",
    "\n",
    "    return scores, boxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Test YOLO pretrained model with image\n",
    "Create a session and start to test on graph\n",
    "- Start a session\n",
    "- Define classes, anchors and image shape. 80 classes and 5 anchors are loaded from files. image_shape need match test image.\n",
    "- Load a pretrained model from \"yolo.h5\" and the summary of model. Model converts input images (shape: (m, 608, 608, 3)) into a tensor of shape (m, 19, 19, 5, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 608, 608, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 608, 608, 32) 864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 608, 608, 32) 128         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 608, 608, 32) 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 304, 304, 32) 0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 304, 304, 64) 18432       max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 304, 304, 64) 256         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 304, 304, 64) 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 152, 152, 64) 0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 152, 152, 128 73728       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 152, 152, 128 512         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 152, 152, 128 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 152, 152, 64) 8192        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 152, 152, 64) 256         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 152, 152, 64) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 152, 152, 128 73728       leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 152, 152, 128 512         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 152, 152, 128 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 76, 76, 128)  0           leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 76, 76, 256)  294912      max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 76, 76, 256)  1024        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 76, 76, 256)  0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 76, 76, 128)  32768       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 76, 76, 128)  512         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 76, 76, 128)  0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 76, 76, 256)  294912      leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 76, 76, 256)  1024        conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 76, 76, 256)  0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 38, 38, 256)  0           leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 38, 38, 512)  1179648     max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 38, 38, 512)  2048        conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 38, 38, 512)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 38, 38, 256)  131072      leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 38, 38, 256)  1024        conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 38, 38, 256)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 38, 38, 512)  1179648     leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 38, 38, 512)  2048        conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 38, 38, 512)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 38, 38, 256)  131072      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 38, 38, 256)  1024        conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 38, 38, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 38, 38, 512)  1179648     leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 38, 38, 512)  2048        conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 38, 38, 512)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 19, 19, 512)  0           leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 19, 19, 1024) 4718592     max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 19, 19, 1024) 4096        conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 19, 19, 512)  524288      leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 19, 19, 512)  2048        conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 19, 19, 512)  0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 19, 19, 1024) 4718592     leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 19, 19, 1024) 4096        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 19, 19, 512)  524288      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 19, 19, 512)  2048        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 19, 19, 512)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 19, 19, 1024) 4718592     leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 19, 19, 1024) 4096        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 19, 19, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 19, 19, 1024) 4096        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 38, 38, 64)   32768       leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 38, 38, 64)   256         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 19, 19, 1024) 9437184     leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 38, 38, 64)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 19, 19, 1024) 4096        conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "space_to_depth_x2 (Lambda)      (None, 19, 19, 256)  0           leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 19, 19, 1280) 0           space_to_depth_x2[0][0]          \n",
      "                                                                 leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 19, 19, 1024) 11796480    concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 19, 19, 1024) 4096        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 19, 19, 1024) 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 19, 19, 425)  435625      leaky_re_lu_22[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 50,983,561\n",
      "Trainable params: 50,962,889\n",
      "Non-trainable params: 20,672\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#start a session\n",
    "sess = K.get_session()\n",
    "\n",
    "# deine classes, anchors and image shape\n",
    "class_names = read_classes(\"model_data/coco_classes.txt\")\n",
    "anchors = read_anchors(\"model_data/yolo_anchors.txt\")\n",
    "image_shape = (1080., 1920.)\n",
    "\n",
    "#load pretrained model\n",
    "yolo_model = load_model(\"model_data/yolo.h5\")\n",
    "#model summary\n",
    "yolo_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process yolo_model.output and get \"yolo_outputs\" which is argument of yolo_eval  \n",
    "Then yolo_eval will perform filtering and select boxes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process yolo_model.output \n",
    "yolo_outputs = yolo_head(yolo_model.output, anchors, len(class_names))\n",
    "# perform filtering and select boxes\n",
    "scores, boxes, classes = yolo_eval(yolo_outputs, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predict with pretrained model:  \n",
    "- yolo_model.input is given to `yolo_model`. The model is used to compute the output yolo_model.output \n",
    "- yolo_model.output is processed by `yolo_head`. It gives you yolo_outputs\n",
    "- yolo_outputs goes through a filtering function, `yolo_eval`. It outputs your predictions: scores, boxes, classes \n",
    "- draw box on image for every selected box"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sess, image_file):\n",
    "    # Preprocess your image\n",
    "    image, image_data = preprocess_image(\"images/\" + image_file, model_image_size = (608, 608))\n",
    "\n",
    "    # Run the session \n",
    "    out_scores, out_boxes, out_classes = sess.run(yolo_eval(yolo_outputs, image_shape),feed_dict={yolo_model.input: image_data, K.learning_phase(): 0})\n",
    "\n",
    "    # Print predictions info\n",
    "    print('Found {} boxes for {}'.format(len(out_boxes), image_file))\n",
    "    # Generate colors for drawing bounding boxes.\n",
    "    colors = generate_colors(class_names)\n",
    "    # Draw bounding boxes on the image file\n",
    "    draw_boxes(image, out_scores, out_boxes, out_classes, class_names, colors)\n",
    "    # Save the predicted bounding box on the image\n",
    "    image.save(os.path.join(\"out\", image_file), quality=90)\n",
    "    # Display the results in the notebook\n",
    "    output_image = scipy.misc.imread(os.path.join(\"out\", image_file))\n",
    "    #imshow(output_image)\n",
    "    \n",
    "    return out_scores, out_boxes, out_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a prediction with image below and print result.\n",
    "\n",
    "\n",
    "<img src=\"images/test.jpg\" style=\"width:640px;height:360;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 boxes for test.jpg\n",
      "car 0.61 (915, 517) (1066, 611)\n",
      "stop sign 0.62 (1507, 428) (1555, 465)\n",
      "car 0.62 (960, 509) (1093, 594)\n",
      "car 0.65 (630, 553) (886, 678)\n",
      "bus 0.81 (17, 363) (568, 935)\n"
     ]
    }
   ],
   "source": [
    "out_scores, out_boxes, out_classes = predict(sess, \"test.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the output image we just predicted. Five boxes including a bus, three cars and a stop signal are detected\n",
    "<img src=\"nb_images/test.jpg\" style=\"width:640px;height:360;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Video Visualization\n",
    "\n",
    "Now it is time to test on video. I got a Video on our BC shuttle bus and will test YOLO model with it. \n",
    "\n",
    "<center>\n",
    "<video width=\"640\" height=\"360\" src=\"images/BC.mov\" type=\"video/mov\" controls>\n",
    "</video>\n",
    "    \n",
    "</center>\n",
    "<caption><center> Vedio 2: Original video taken by Chaobin Yang from iphone hold on Boston College shuttle bus while driving around Boston College. \n",
    "</center></caption>\n",
    "\n",
    "### 4.1 break video to many images\n",
    "First, break down the video into images. Write images into folder \"images\". Do so until success becomes false or 1197 images are got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidcap = cv2.VideoCapture('images/BC.mov')\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "while success:\n",
    "    cv2.imwrite(\"images/BC%d.jpg\" % count, image)     # save frame as JPEG file      \n",
    "    success,image = vidcap.read()\n",
    "    count += 1\n",
    "    if count>1197: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Predict every image\n",
    "Second, predict every images come from video. Output result into \"out\" folder. I only choose the first 1197 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=list()\n",
    "for i in range(1198):\n",
    "    images.append(\"BC\"+str(i)+\".jpg\")\n",
    "    out_scores, out_boxes, out_classes = predict(sess, \"BC\"+str(i)+\".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Merge predicted images to one video\n",
    "use moviepy package. and concatenate all predicted images into a video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video Result_BC.mp4\n",
      "[MoviePy] Writing video Result_BC.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 72/72 [00:01<00:00, 38.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: Result_BC.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "\n",
    "# get every images for video\n",
    "clips = [ImageClip(\"out\\\\\"+ m).set_duration(0.03) for m in images]\n",
    "\n",
    "#concatenate images to a video\n",
    "concat_clip = concatenate_videoclips(clips, method=\"compose\")\n",
    "\n",
    "# output video to disk\n",
    "concat_clip.write_videofile(\"Result_BC.mp4\", fps=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output video \"Result_BC.mp4\" is vedeo 1 we shown in the beginning  \n",
    "\n",
    "<center>\n",
    "<video width=\"640\" height=\"360\" src=\"nb_images/Result_BC.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 References\n",
    "\n",
    "[1] The official YOLO website (https://pjreddie.com/darknet/yolo/)  \n",
    "[2] Allan Zelener - [YAD2K: Yet Another Darknet 2 Keras](https://github.com/allanzelener/YAD2K)  \n",
    "[3] Joseph Redmon, Santosh Divvala, Ross Girshick, Ali Farhadi - [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) (2015)  \n",
    "[4] Joseph Redmon, Ali Farhadi - [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) (2016)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
